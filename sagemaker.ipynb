{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of the TODO's and/or use more than one cell to complete all the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tho121/udacity_aws/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import any packages that you might need\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.pytorch import PyTorch\n",
    "#from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "**TODO:** Run the cell below to download the data.\n",
    "\n",
    "The cell below creates a folder called `train_data`, downloads training data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder. For instance, all images in folder `1` has images with 1 object in them. Images are not divided into training, testing or validation sets. If you feel like the number of samples are not enough, you can always download more data (instructions for that can be found [here](https://registry.opendata.aws/amazon-bin-imagery/)). However, we are not acessing you on the accuracy of your final trained model, but how you create your machine learning engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Images with 1 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25987/25987 [38:54<00:00, 11.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Images with 2 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48103/48103 [1:13:14<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Images with 3 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56409/56409 [1:27:47<00:00, 10.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Images with 4 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50384/50384 [1:17:35<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Images with 5 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39117/39117 [1:01:22<00:00, 10.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_and_arrange_data():\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    with open('large_list.json', 'r') as f:\n",
    "        d=json.load(f)\n",
    "\n",
    "    for k, v in d.items():\n",
    "        print(f\"Downloading Images with {k} objects\")\n",
    "        directory=os.path.join('large_train_data', k)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        for file_path in tqdm(v):\n",
    "            file_name=os.path.basename(file_path).split('.')[0]+'.jpg'\n",
    "            s3_client.download_file('aft-vbi-pds', os.path.join('bin-images', file_name),\n",
    "                             os.path.join(directory, file_name))\n",
    "\n",
    "download_and_arrange_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "**TODO:** Explain what dataset you are using for this project. Give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understanding of it. You can find more information about the data [here](https://registry.opendata.aws/amazon-bin-imagery/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1229\n",
      "2300\n",
      "2666\n",
      "2373\n",
      "1875\n"
     ]
    }
   ],
   "source": [
    "#TODO: Perform any data cleaning or data preprocessing\n",
    "print(len(os.listdir('large_train_data/1')))\n",
    "print(len(os.listdir('large_train_data/2')))\n",
    "print(len(os.listdir('large_train_data/3')))\n",
    "print(len(os.listdir('large_train_data/4')))\n",
    "print(len(os.listdir('large_train_data/5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\"large_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.1 * len(dataset))\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [len(dataset) - test_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"resized_train\")\n",
    "os.mkdir(\"resized_train/1\")\n",
    "os.mkdir(\"resized_train/2\")\n",
    "os.mkdir(\"resized_train/3\")\n",
    "os.mkdir(\"resized_train/4\")\n",
    "os.mkdir(\"resized_train/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"resized_test\")\n",
    "os.mkdir(\"resized_test/1\")\n",
    "os.mkdir(\"resized_test/2\")\n",
    "os.mkdir(\"resized_test/3\")\n",
    "os.mkdir(\"resized_test/4\")\n",
    "os.mkdir(\"resized_test/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for img, label in train_data:\n",
    "    img.save(os.path.join(\"large_train\", str(label + 1), str(count) +\".jpg\"))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for img, label in test_data:\n",
    "    img.save(os.path.join(\"large_test\", str(label + 1), str(count) +\".jpg\"))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Upload the data to AWS S3\n",
    "\n",
    "s3_path_to_data = sagemaker.Session().upload_data(bucket='bincapstone', \n",
    "                                                  path='large_train', \n",
    "                                                  key_prefix='capstone/data/large_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path_to_data_test = sagemaker.Session().upload_data(bucket='bincapstone', \n",
    "                                                  path='large_test', \n",
    "                                                  key_prefix='capstone/data/large_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file('aft-vbi-pds', os.path.join('metadata', \"101598.json\"),\n",
    "                             os.path.join(\"\", \"101598.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_bucket = s3.Bucket('aft-vbi-pds')\n",
    "\n",
    "#arn:aws:s3:::aft-vbi-pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket='aft-vbi-pds'\n",
    "\n",
    "with open('full_list.json','r+') as file:\n",
    "    file_data = json.load(file)\n",
    "    \n",
    "    \n",
    "count = 0\n",
    "for object_summary in data_bucket.objects.filter(Prefix=\"metadata\"):\n",
    "    key = object_summary.key\n",
    "    if key.endswith(\".json\"):\n",
    "        data_location = 's3://{}/{}'.format(bucket, key)\n",
    "        \n",
    "        data = pd.read_json(data_location) \n",
    "\n",
    "        num = data['EXPECTED_QUANTITY']\n",
    "        \n",
    "        for n in num:\n",
    "            if n > 0 and n < 6:\n",
    "                file_data[str(n)].append(key)\n",
    "                count += 1\n",
    "                \n",
    "                if count % 10000 == 0:\n",
    "                    with open('check.json', 'w+') as file:\n",
    "                        json.dump(file_data, file)\n",
    "                    print(count)\n",
    "            break\n",
    "        \n",
    "with open('check.json','w+') as file:\n",
    "    json.dump(file_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.client('s3')\n",
    "paginator = s3_resource.get_paginator('list_objects_v2')\n",
    "page_iterator = paginator.paginate(Bucket='aft-vbi-pds')\n",
    "keys=[]\n",
    "for page in page_iterator:\n",
    "    for i in page['Contents']:\n",
    "        keys.append(i['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_iterator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Boto3 Resource to do the copy\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = s3_resource.Bucket('s3://bincapstone/capstone/data/all/')\n",
    "for key in keys:\n",
    "    copy_source = {\n",
    "        'Bucket': 'arn:aws:s3:::aft-vbi-pds',\n",
    "        'Key': key\n",
    "    }\n",
    "    \n",
    "    bucket.copy(copy_source, key)\n",
    "    \n",
    "    file_name=os.path.basename(file_path).split('.')[0]+'.jpg'\n",
    "    s3_client.download_file('aft-vbi-pds', os.path.join('bin-images', file_name), os.path.join(directory, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "transforms = T.Compose([T.Resize((224, 224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(\"large_train\", transform=transforms)\n",
    "test_dataset = torchvision.datasets.ImageFolder(\"large_test\", transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for img, label in train_dataset:\n",
    "    img.save(os.path.join(\"resized_train\", str(label + 1), str(count) +\".jpg\"))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for img, label in test_dataset:\n",
    "    img.save(os.path.join(\"resized_test\", str(label + 1), str(count) +\".jpg\"))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path_to_data = sagemaker.Session().upload_data(bucket='bincapstone', \n",
    "                                                  path='resized_train', \n",
    "                                                  key_prefix='capstone/data/resized_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path_to_data = sagemaker.Session().upload_data(bucket='bincapstone', \n",
    "                                                  path='resized_test', \n",
    "                                                  key_prefix='capstone/data/resized_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=1)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mean_std(loader):\n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        b,c,h,w = images.shape\n",
    "        nb_pixels = b*h*w\n",
    "        sum_ = torch.sum(images, dim=[0,2,3])\n",
    "        sum_of_squares = torch.sum(images **2, dim=[0,2,3])\n",
    "        \n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_squares) / (cnt + nb_pixels)\n",
    "        \n",
    "    mean = fst_moment\n",
    "    std = torch.sqrt(snd_moment - fst_moment ** 2)\n",
    "    return mean, std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5300, 0.4495, 0.3624])\n",
      "tensor([0.1691, 0.1476, 0.1114])\n"
     ]
    }
   ],
   "source": [
    "mean,std = batch_mean_std(train_data_loader)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5206, 0.4400, 0.3570])\n",
      "tensor([0.1658, 0.1467, 0.1113])\n"
     ]
    }
   ],
   "source": [
    "mean,std = batch_mean_std(test_data_loader)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "**TODO:** This is the part where you can train a model. The type or architecture of the model you use is not important. \n",
    "\n",
    "**Note:** You will need to use the `train.py` script to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name Tony to get Role path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current AWS identity is not a role: arn:aws:iam::149633813601:user/Tony, therefore it cannot be used as a SageMaker execution role",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m \u001b[39mimport\u001b[39;00m get_execution_role\n\u001b[0;32m----> 2\u001b[0m role \u001b[39m=\u001b[39m get_execution_role()\n",
      "File \u001b[0;32m~/udacity_aws/venv/lib/python3.8/site-packages/sagemaker/session.py:4917\u001b[0m, in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   4912\u001b[0m     \u001b[39mreturn\u001b[39;00m arn\n\u001b[1;32m   4913\u001b[0m message \u001b[39m=\u001b[39m (\n\u001b[1;32m   4914\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe current AWS identity is not a role: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, therefore it cannot be used as a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4915\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSageMaker execution role\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4916\u001b[0m )\n\u001b[0;32m-> 4917\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message\u001b[39m.\u001b[39mformat(arn))\n",
      "\u001b[0;31mValueError\u001b[0m: The current AWS identity is not a role: arn:aws:iam::149633813601:user/Tony, therefore it cannot be used as a SageMaker execution role"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name Tony to get Role path.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='SagemakerFullAccess')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare your model training hyperparameter.\n",
    "hyperparameters = {'batch_size': 128, 'lr': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your training estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    base_job_name='capstone',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    framework_version='1.12.0',\n",
    "    py_version='py38',\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-02 21:59:29 Starting - Starting the training job...\n",
      "2022-12-02 22:00:00 Starting - Preparing the instances for trainingProfilerReport-1670018374: InProgress\n",
      "......\n",
      "2022-12-02 22:01:00 Downloading - Downloading input data...........bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-12-02 22:02:48,943 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-12-02 22:02:48,945 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:48,953 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-12-02 22:02:48,959 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\n",
      "2022-12-02 22:02:48,959 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-12-02 22:02:49,461 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:49,471 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:49,480 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "2022-12-02 22:02:49,480 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "2022-12-02 22:02:49,483 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "2022-12-02 22:02:49,485 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\n",
      "2022-12-02 22:02:49,485 sagemaker-training-toolkit INFO     Connection closed\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-12-02 22:02:50,064 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-12-02 22:02:50,066 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:50,074 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-12-02 22:02:50,081 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\n",
      "2022-12-02 22:02:50,081 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-12-02 22:02:50,487 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\n",
      "2022-12-02 22:02:50,487 sagemaker-training-toolkit INFO     Connection closed\n",
      "2022-12-02 22:02:50,527 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:50,537 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:50,546 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "2022-12-02 22:02:50,546 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\n",
      "2022-12-02 22:02:50,556 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "2022-12-02 22:02:50,642 paramiko.transport INFO     Authentication (publickey) successful!\n",
      "2022-12-02 22:02:50,643 sagemaker-training-toolkit INFO     Can connect to host algo-1\n",
      "2022-12-02 22:02:50,643 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\n",
      "2022-12-02 22:02:50,643 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\n",
      "2022-12-02 22:02:50,647 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\n",
      "2022-12-02 22:02:51,499 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "2022-12-02 22:02:51,583 paramiko.transport INFO     Authentication (publickey) successful!\n",
      "2022-12-02 22:02:51,584 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\n",
      "2022-12-02 22:02:51,584 sagemaker-training-toolkit INFO     Connection closed\n",
      "2022-12-02 22:02:51,584 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\n",
      "2022-12-02 22:02:51,584 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "2022-12-02 22:02:51,584 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\n",
      "2022-12-02 22:02:51,588 sagemaker-training-toolkit INFO     instance type: ml.m5.4xlarge\n",
      "2022-12-02 22:02:51,588 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:1', 'algo-2:1'] process_per_hosts: 1 num_processes: 2\n",
      "2022-12-02 22:02:51,591 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2022-12-02 22:02:51,599 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.m5.4xlarge\",\n",
      "        \"sagemaker_pytorch_ddp_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.4xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"lr\": 0.001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"capstone-2022-12-02-21-59-33-474\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-149633813601/capstone-2022-12-02-21-59-33-474/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch_size\":128,\"lr\":0.001}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.m5.4xlarge\",\"sagemaker_pytorch_ddp_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.4xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.m5.4xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=16\n",
      "SM_NUM_GPUS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-149633813601/capstone-2022-12-02-21-59-33-474/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.m5.4xlarge\",\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.m5.4xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":128,\"lr\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"capstone-2022-12-02-21-59-33-474\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-149633813601/capstone-2022-12-02-21-59-33-474/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.4xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--batch_size\",\"128\",\"--lr\",\"0.001\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_BATCH_SIZE=128\n",
      "SM_HP_LR=0.001\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\n",
      "Invoking script with the following command:\n",
      "mpirun --host algo-1:1,algo-2:1 -np 2 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.m5.4xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py train.py --batch_size 128 --lr 0.001\n",
      "Warning: Permanently added 'algo-2,10.0.178.62' (ECDSA) to the list of known hosts.\n",
      "--------------------------------------------------------------------------\n",
      "mpirun was unable to find the specified executable file, and therefore\n",
      "did not launch the job.  This error was first reported for process\n",
      "rank 0; it may have occurred for other processes as well.\n",
      "NOTE: A common cause for this error is misspelling a mpirun command\n",
      "      line parameter option (remember that mpirun interprets the first\n",
      "      unrecognized command line token as the executable).\n",
      "Node:       algo-1\n",
      "Executable: smddprun\n",
      "--------------------------------------------------------------------------\n",
      "2 total processes failed to start\n",
      "2022-12-02 22:02:52,070 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2022-12-02 22:02:52,070 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 134 from exiting process.\n",
      "2022-12-02 22:02:52,071 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2022-12-02 22:02:52,071 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 134\n",
      "ErrorMessage \"\"\n",
      "Command \"mpirun --host algo-1:1,algo-2:1 -np 2 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.m5.4xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py train.py --batch_size 128 --lr 0.001\"\n",
      "2022-12-02 22:02:52,071 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "\n",
      "2022-12-02 22:03:08 Training - Training image download completed. Training in progress.\n",
      "2022-12-02 22:03:08 Uploading - Uploading generated training model\n",
      "2022-12-02 22:03:08 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job capstone-2022-12-02-21-59-33-474: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \"\"\nCommand \"mpirun --host algo-1:1,algo-2:1 -np 2 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.m5.4xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py train.py --batch_size 128 --lr 0.001\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mSM_CHANNEL_TEST\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://awscapstone/test_data/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mSM_MODEL_DIR\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://awscapstone/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m estimator\u001b[39m.\u001b[39;49mfit({\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mSM_CHANNEL_TRAIN\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m: os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mSM_CHANNEL_TEST\u001b[39;49m\u001b[39m'\u001b[39;49m] }, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/udacity_aws/venv/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:272\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    270\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/udacity_aws/venv/lib/python3.8/site-packages/sagemaker/estimator.py:1128\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1127\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 1128\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/udacity_aws/venv/lib/python3.8/site-packages/sagemaker/estimator.py:2242\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2241\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   2243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/udacity_aws/venv/lib/python3.8/site-packages/sagemaker/session.py:4072\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4069\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   4071\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 4072\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   4073\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   4074\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/udacity_aws/venv/lib/python3.8/site-packages/sagemaker/session.py:3603\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   3598\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   3599\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   3600\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   3601\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   3602\u001b[0m     )\n\u001b[0;32m-> 3603\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3604\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   3605\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   3606\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   3607\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job capstone-2022-12-02-21-59-33-474: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \"\"\nCommand \"mpirun --host algo-1:1,algo-2:1 -np 2 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.m5.4xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py train.py --batch_size 128 --lr 0.001\", exit code: 1"
     ]
    }
   ],
   "source": [
    "# TODO: Fit your estimator\n",
    "os.environ['SM_CHANNEL_TRAIN']='s3://awscapstone/train_data/'\n",
    "os.environ['SM_CHANNEL_TEST']='s3://awscapstone/test_data/'\n",
    "os.environ['SM_MODEL_DIR']='s3://awscapstone/'\n",
    "estimator.fit({\"train\": os.environ['SM_CHANNEL_TRAIN'], \"test\": os.environ['SM_CHANNEL_TEST'] }, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standout Suggestions\n",
    "You do not need to perform the tasks below to finish your project. However, you can attempt these tasks to turn your project into a more advanced portfolio piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "**TODO:** Here you can perform hyperparameter tuning to increase the performance of your model. You are encouraged to \n",
    "- tune as many hyperparameters as you can to get the best performance from your model\n",
    "- explain why you chose to tune those particular hyperparameters and the ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your training estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit your estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling and Debugging\n",
    "**TODO:** Use model debugging and profiling to better monitor and debug your model training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up debugging and profiling rules and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a debugging output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deploying and Querying\n",
    "**TODO:** Can you deploy your model to an endpoint and then query that endpoint to get a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaper Training and Cost Analysis\n",
    "**TODO:** Can you perform a cost analysis of your system and then use spot instances to lessen your model training cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model using a spot instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Instance Training\n",
    "**TODO:** Can you train your model on multiple instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model on Multiple Instances"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b9a16027b1face11683377a0f72853ed9096586e91733d0ba9340678d80f683"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
